"""Evaluate summary quality: ROUGE, BERTScore, length, compression, redundancy."""
from __future__ import annotations
import argparse, json, csv
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict
import re

try:
    from rouge_score import rouge_scorer
    from bert_score import score as bert_score
except ImportError:
    print("[WARNING] Install dependencies: pip install rouge-score bert-score")
    rouge_scorer = None
    bert_score = None


def load_jsonl(fp: Path) -> List[Dict]:
    rows = []
    with fp.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                rows.append(json.loads(line))
    return rows


def write_jsonl(fp: Path, rows: List[Dict]) -> None:
    fp.parent.mkdir(parents=True, exist_ok=True)
    with fp.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def write_csv(fp: Path, rows: List[Dict]) -> None:
    if not rows:
        return
    fp.parent.mkdir(parents=True, exist_ok=True)
    keys = rows[0].keys()
    with fp.open("w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=keys)
        w.writeheader()
        w.writerows(rows)


def tokenize_simple(text: str) -> List[str]:
    """Simple whitespace + punct tokenizer for length counting."""
    return re.findall(r"\w+", text.lower())


def compute_redundancy(summary: str, threshold: float = 0.85) -> float:
    """Fraction of sentence pairs with high Jaccard similarity."""
    sents = re.split(r"[.!?]+", summary)
    sents = [s.strip() for s in sents if s.strip()]
    if len(sents) < 2:
        return 0.0
    
    pairs = 0
    redundant = 0
    for i in range(len(sents)):
        for j in range(i+1, len(sents)):
            pairs += 1
            words_i = set(tokenize_simple(sents[i]))
            words_j = set(tokenize_simple(sents[j]))
            if not words_i or not words_j:
                continue
            jaccard = len(words_i & words_j) / len(words_i | words_j)
            if jaccard >= threshold:
                redundant += 1
    return redundant / pairs if pairs > 0 else 0.0


def evaluate_summary(
    generated: str,
    reference: str,
    source_tokens: int,
    target_length: int = 120,
) -> Dict[str, float]:
    """Compute metrics for a single summary."""
    metrics: Dict[str, float] = {}
    
    gen_tokens = tokenize_simple(generated)
    ref_tokens = tokenize_simple(reference)
    
    metrics["length_tokens"] = len(gen_tokens)
    metrics["length_deviation"] = abs(len(gen_tokens) - target_length)
    metrics["compression_ratio"] = source_tokens / max(1, len(gen_tokens))
    metrics["redundancy_rate"] = compute_redundancy(generated)
    
    # ROUGE
    if rouge_scorer:
        scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
        scores = scorer.score(reference, generated)
        metrics["rouge1_f"] = scores["rouge1"].fmeasure
        metrics["rouge2_f"] = scores["rouge2"].fmeasure
        metrics["rougeL_f"] = scores["rougeL"].fmeasure
    else:
        metrics["rouge1_f"] = metrics["rouge2_f"] = metrics["rougeL_f"] = -1.0
    
    return metrics


def evaluate_batch_bertscore(generated_list: List[str], reference_list: List[str]) -> List[float]:
    """Compute BERTScore F1 for batch (faster than one-by-one)."""
    if not bert_score:
        return [-1.0] * len(generated_list)
    P, R, F1 = bert_score(generated_list, reference_list, lang="en", verbose=False)
    return F1.tolist()


def main() -> None:
    p = argparse.ArgumentParser(description="Evaluate summary quality metrics.")
    p.add_argument("--gold", required=True, type=Path, help="Path to gold_summaries.jsonl")
    p.add_argument("--generated", required=True, type=Path, help="Path to generated_summaries.jsonl")
    p.add_argument("--out-detail", default="data/eval/summary_metrics_detail.jsonl", type=Path)
    p.add_argument("--out-agg", default="data/eval/summary_metrics.csv", type=Path)
    p.add_argument("--target-length", default=120, type=int, help="Target summary length in tokens")
    args = p.parse_args()
    
    gold_rows = load_jsonl(args.gold)
    gen_rows = load_jsonl(args.generated)
    
    # Index gold by query_id
    gold_map = {r["query_id"]: r for r in gold_rows}
    
    # Group generated by system_variant
    by_system: Dict[str, List[Dict]] = defaultdict(list)
    for g in gen_rows:
        by_system[g["system_variant"]].append(g)
    
    detail_rows = []
    agg_rows = []
    
    for system_variant, sys_gen in by_system.items():
        # Align with gold
        pairs = []
        for g in sys_gen:
            qid = g["query_id"]
            if qid not in gold_map:
                print(f"[WARN] query_id={qid} not in gold, skipping")
                continue
            gold = gold_map[qid]
            pairs.append((g, gold))
        
        if not pairs:
            print(f"[WARN] system_variant={system_variant} has no valid pairs")
            continue
        
        # Compute per-query metrics
        per_query_metrics = []
        gen_texts = []
        ref_texts = []
        for g, gold in pairs:
            gen_text = g["generated_summary"]
            ref_text = gold["reference_summary"]
            source_tokens = g.get("source_tokens", 1000)  # fallback
            
            m = evaluate_summary(gen_text, ref_text, source_tokens, args.target_length)
            m["query_id"] = g["query_id"]
            m["system_variant"] = system_variant
            per_query_metrics.append(m)
            
            gen_texts.append(gen_text)
            ref_texts.append(ref_text)
        
        # Batch BERTScore
        bert_scores = evaluate_batch_bertscore(gen_texts, ref_texts)
        for i, m in enumerate(per_query_metrics):
            m["bertscore_f1"] = bert_scores[i]
        
        detail_rows.extend(per_query_metrics)
        
        # Aggregate
        agg = {"system_variant": system_variant, "n_queries": len(per_query_metrics)}
        for key in ["rouge1_f", "rouge2_f", "rougeL_f", "bertscore_f1", 
                    "length_deviation", "compression_ratio", "redundancy_rate"]:
            vals = [m[key] for m in per_query_metrics if m[key] >= 0]
            agg[f"{key}_mean"] = sum(vals) / len(vals) if vals else 0.0
        agg_rows.append(agg)
    
    write_jsonl(args.out_detail, detail_rows)
    write_csv(args.out_agg, agg_rows)
    
    print(f"[DONE] Evaluated {len(detail_rows)} query√ósystem pairs")
    print(f"  Detail: {args.out_detail}")
    print(f"  Aggregate: {args.out_agg}")


if __name__ == "__main__":
    main()
