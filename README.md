# RAG-Powered News Assistant ğŸš€ğŸ“°

This project builds a full-stack pipeline to scrape, clean, summarize, and recommend tech/finance news using Retrieval-Augmented Generation (RAG).

## ğŸš€ Quick Start

### Installation

1. **Clone the repository:**
   ```bash
   git clone <your-repo-url>
   cd tech-news-summarizer
   ```

2. **Create and activate virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“¥ Rebuilding Scraped Data

This project does **not** ship full datasets or vector stores in the repo (to keep it lightweight and respect site content licenses).  
Instead, you can reproduce everything locally with the provided data pipeline.

### 1. Configure Sources
Edit [`config/feeds.yml`](config/feeds.yml) to specify which news sites to scrape:

```yaml
sources:
  - domain: https://techcrunch.com
    limit: 20
    language: en
  - domain: https://www.theverge.com
    limit: 10
    language: en
```

### 2. Run the pipeline (end-to-end)
From the repo root:

```
# Scrape â†’ Clean â†’ Index into Chroma
python -m data_pipeline.pipeline all \
  --feeds config/feeds.yml \
  --rag-cfg config/rag.yml \
  --ticker-cfg config/tickers.yml
```
This will generate:

- Raw scraped data â†’ data/raw/articles.jsonl
- Cleaned & preprocessed data â†’ data/processed/articles_clean.csv/jsonl, data/processed/preprocessed.jsonl
- Vector database (Chroma) â†’ data/vdb/chroma/
- Analytics artifacts (JSON/Parquet) â†’ data/analytics/

### 3. Run Stages Individually (Optional)

```
# Scrape raw articles
python -m data_pipeline.pipeline scrape \
  --config config/feeds.yml \
  --out-jsonl data/raw/articles.jsonl

# Clean + preprocess
python -m data_pipeline.pipeline clean \
  --in-jsonl data/raw/articles.jsonl \
  --out-csv data/processed/articles_clean.csv \
  --out-jsonl data/processed/articles_clean.jsonl \
  --out-pre-jsonl data/processed/preprocessed.jsonl \
  --make-chunks

# Index into Chroma
python -m data_pipeline.pipeline index \
  --input data/processed/articles_clean.jsonl \
  --cfg config/rag.yml

# Analytics metrics + dashboards
python -m data_pipeline.pipeline analytics \
  --rag-cfg config/rag.yml \
  --ticker-cfg config/tickers.yml \
  --out-dir data/analytics
```

### 4. Verify Outputs

- Raw data: data/raw/
- Cleaned data: data/processed/
- Vector DB: data/vdb/chroma/
- Analytics cache & metrics: data/analytics/

## ğŸ“ˆ Analytics & Dashboard

1. **Regenerate metrics** (after indexing or when new articles arrive):
   ```bash
   python -m data_pipeline.pipeline analytics \
     --rag-cfg config/rag.yml \
     --ticker-cfg config/tickers.yml \
     --out-dir data/analytics
   ```
   This will
   - Extract article metadata from Chroma with ticker/sector enrichment.
   - Score sentiment per article (cached at `data/analytics/sentiment.parquet`).
   - Persist sector sentiment, ticker mentions, and co-occurrence matrices as JSON + Parquet under `data/analytics/`.

2. **Expose analytics via FastAPI**: set `ANALYTICS_DIR` if you keep metrics outside the default.
   ```bash
   export ANALYTICS_DIR=/path/to/data/analytics
   uvicorn src.api.api_main:app --host 0.0.0.0 --port 8000 --reload
   ```

3. **Launch the Streamlit analytics dashboard**:
   ```bash
   streamlit run app/dashboard_streamlit.py --server.port 8502
   ```
   Use the sidebar controls to adjust date/ticker filters. The charts call the FastAPI analytics endpoints (`/analytics/overview`, `/analytics/cooccurrence`).

## ğŸš€ How to run FastAPI + Streamlit with Chroma:

### 1. Start the FastAPI Backend
```
# open a terminal and run
uvicorn src.api.api_main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. Start the Streamlit Frontend
```
# Open a second terminal and run:
streamlit run app/ui_streamlit.py --server.port 8501
```

## ğŸ“Œ Project Goals
- Scrape news from sources like TechCrunch, Wired, and The Verge
- Summarize and making recommendation with RAG
- Deploy via Streamlit with interactive UI
- Visualize insights using Tableau

## ğŸ—‚ï¸ Project Structure
```text
tech-news-summarizer/
â”œâ”€â”€ app/                      # Frontend/UI
â”‚   â””â”€â”€ ui_streamlit.py
â”‚
â”œâ”€â”€ data_pipeline/            # Data prep & indexing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scrape.py             # newspaper3k scraping
â”‚   â”œâ”€â”€ clean.py              # cleaning & normalization
â”‚   â”œâ”€â”€ embed.py              # embeddings + OpenAI API
â”‚   â”œâ”€â”€ index_chroma.py       # ingest into Chroma/FAISS
â”‚   â””â”€â”€ pipeline.py           # orchestrate full pipeline
â”‚
â”œâ”€â”€ src/                      # Core backend logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag/                  # Retrieval + RAG
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ ranker.py
â”‚   â”‚   â”œâ”€â”€ rag_summarizer.py
â”‚   â”‚   â””â”€â”€ recommend.py
â”‚   â”œâ”€â”€ api/                  # API layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI app
â”‚   â””â”€â”€ schemas.py            # Pydantic models shared across RAG/API
â”‚
â”œâ”€â”€ config/                   # YAML configs
â”‚   â”œâ”€â”€ feeds.yml
â”‚   â””â”€â”€ rag.yml
â”‚
â”œâ”€â”€ data/                     # Local storage (gitignored)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ chunks/
â”‚   â””â”€â”€ vdb/
â”‚
â”œâ”€â”€ tests/                    # Unit + E2E tests
â”œâ”€â”€ notebooks/                # EDA & experiments
â”œâ”€â”€ cli.py                    # optional: Typer CLI as single entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore

```

## ğŸš§ Project Milestones

| Week | Milestone |
|------|-----------|
| âœ… Week 1 | Scraped and cleaned tech articles |
| âœ… Week 2 | Built and evaluated LLM summarizer |
| âœ… Week 3 | RAG-based improvement with Chroma |
| âœ… Week 4 | RAG-based recommendation |
| âœ… Week 5 | Streamlit app deployment + demo |

