# LLM-Powered Tech News Summarizer ğŸš€ğŸ“°

This project builds a full-stack pipeline to scrape, clean, summarize, and recommend tech/finance news using Retrieval-Augmented Generation (RAG).

## ğŸš€ Quick Start

### Installation

1. **Clone the repository:**
   ```bash
   git clone <your-repo-url>
   cd tech-news-summarizer
   ```

2. **Create and activate virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“¥ Rebuilding Scraped Data

This project does **not** ship full datasets or vector stores in the repo (to keep it lightweight and respect site content licenses).  
Instead, you can reproduce everything locally with the provided data pipeline.

### 1. Configure Sources
Edit [`config/feeds.yml`](config/feeds.yml) to specify which news sites to scrape:

```yaml
sources:
  - domain: https://techcrunch.com
    limit: 20
    language: en
  - domain: https://www.theverge.com
    limit: 10
    language: en
```

### 2. Run the pipeline (end-to-end)
From the repo root: 


```
# Scrape â†’ Clean â†’ Index into Chroma
python -m data_pipeline.pipeline all \
  --feeds config/feeds.yml \
  --rag-cfg config/rag.yml
```


## ğŸ“Œ Project Goals
- Scrape news from sources like TechCrunch, Wired, and The Verge
- Summarize and making recommendation with RAG
- Deploy via Streamlit with interactive UI
- Visualize insights using Tableau

## ğŸ—‚ï¸ Project Structure
```text
tech-news-summarizer/
â”œâ”€â”€ app/                      # Frontend/UI
â”‚   â””â”€â”€ ui_streamlit.py
â”‚
â”œâ”€â”€ data_pipeline/            # Data prep & indexing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scrape.py             # newspaper3k scraping
â”‚   â”œâ”€â”€ clean.py              # cleaning & normalization
â”‚   â”œâ”€â”€ embed.py              # embeddings + OpenAI API
â”‚   â”œâ”€â”€ index_chroma.py       # ingest into Chroma/FAISS
â”‚   â””â”€â”€ pipeline.py           # orchestrate full pipeline
â”‚
â”œâ”€â”€ src/                      # Core backend logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag/                  # Retrieval + RAG
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ ranker.py
â”‚   â”‚   â”œâ”€â”€ rag_summarizer.py
â”‚   â”‚   â””â”€â”€ recommend.py
â”‚   â”œâ”€â”€ api/                  # API layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI app
â”‚   â””â”€â”€ schemas.py            # Pydantic models shared across RAG/API
â”‚
â”œâ”€â”€ config/                   # YAML configs
â”‚   â”œâ”€â”€ feeds.yml
â”‚   â””â”€â”€ rag.yml
â”‚
â”œâ”€â”€ data/                     # Local storage (gitignored)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ chunks/
â”‚   â””â”€â”€ vdb/
â”‚
â”œâ”€â”€ tests/                    # Unit + E2E tests
â”œâ”€â”€ notebooks/                # EDA & experiments
â”œâ”€â”€ cli.py                    # optional: Typer CLI as single entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore

```

## ğŸš§ Project Milestones

| Week | Milestone |
|------|-----------|
| âœ… Week 1 | Scraped and cleaned tech articles |
| âœ… Week 2 | Built and evaluated LLM summarizer |
| âœ… Week 3 | RAG-based improvement with Chroma |
| âœ… Week 4 | RAG-based recommendation |
| âœ… Week 5 | Streamlit app deployment + demo |

