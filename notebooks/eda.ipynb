{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0729d976",
   "metadata": {},
   "source": [
    "# EDA: Cleaned Articles Quality Check\n",
    "\n",
    "This notebook validates cleaned articles before indexing into ChromaDB.\n",
    "\n",
    "**Pipeline Stage:** Between `clean.py` ‚Üí `index.py`\n",
    "\n",
    "**Validation Areas:**\n",
    "1. Schema completeness\n",
    "2. Text quality metrics\n",
    "3. Temporal coverage\n",
    "4. Source distribution\n",
    "5. Language validation\n",
    "6. Chunking preview\n",
    "7. URL integrity\n",
    "8. Content hash verification\n",
    "9. Metadata richness\n",
    "10. Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babaf72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee155e",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned JSONL\n",
    "data_path = Path('../data/processed/articles_clean.jsonl')\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} articles\")\n",
    "print(f\"‚úì Columns: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45917806",
   "metadata": {},
   "source": [
    "## 2. Schema Validation (¬ß15: Metadata Schema Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required fields (per ¬ß15: Metadata Schema Enforcement)\n",
    "required_fields = ['id', 'url', 'source', 'title', 'text', 'published_at', 'language']\n",
    "optional_fields = ['authors', 'content_hash']\n",
    "\n",
    "print(\"=== Required Fields Completeness ===\")\n",
    "missing_counts = df[required_fields].isnull().sum()\n",
    "print(missing_counts)\n",
    "print(f\"\\n‚úì Articles with all required fields: {(missing_counts == 0).all()}\")\n",
    "\n",
    "# Check for empty strings (not just null)\n",
    "empty_strings = df[required_fields].apply(lambda x: (x == '').sum() if x.dtype == 'object' else 0)\n",
    "if empty_strings.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Found {empty_strings.sum()} empty string values\")\n",
    "    print(empty_strings[empty_strings > 0])\n",
    "\n",
    "# Data type validation\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(f\"‚úì published_at is string: {df['published_at'].dtype == 'object'}\")\n",
    "print(f\"‚úì language is string: {df['language'].dtype == 'object'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d41cb",
   "metadata": {},
   "source": [
    "## 3. Text Quality Assessment (¬ß12: Deduplication Early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ed696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text metrics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"=== Text Length Statistics ===\")\n",
    "print(df['text_length'].describe())\n",
    "print(f\"\\nMedian text length: {df['text_length'].median():.0f} chars\")\n",
    "print(f\"Median word count: {df['word_count'].median():.0f} words\")\n",
    "\n",
    "# Flag problematic articles (per ¬ß12 validation rules)\n",
    "short_articles = df[df['text_length'] < 300]\n",
    "low_word_count = df[df['word_count'] < 80]\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Articles <300 chars: {len(short_articles)} ({len(short_articles)/len(df)*100:.1f}%)\")\n",
    "print(f\"‚ö†Ô∏è Articles <80 words: {len(low_word_count)} ({len(low_word_count)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for duplicates by content_hash\n",
    "if 'content_hash' in df.columns:\n",
    "    duplicate_hashes = df['content_hash'].duplicated().sum()\n",
    "    print(f\"\\n‚úì Duplicate content_hash: {duplicate_hashes}\")\n",
    "    \n",
    "# Distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "df['text_length'].hist(bins=30, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].axvline(300, color='red', linestyle='--', label='Min threshold (300)')\n",
    "axes[0].legend()\n",
    "\n",
    "df['word_count'].hist(bins=30, ax=axes[1], edgecolor='black')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].axvline(80, color='red', linestyle='--', label='Min threshold (80)')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ff391",
   "metadata": {},
   "source": [
    "## 4. Temporal Coverage & Recency (¬ß11: Metadata-First Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e78d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df['published_dt'] = pd.to_datetime(df['published_at'], errors='coerce')\n",
    "\n",
    "print(\"=== Temporal Coverage ===\")\n",
    "date_range = df['published_dt'].agg(['min', 'max'])\n",
    "print(f\"Earliest article: {date_range['min']}\")\n",
    "print(f\"Latest article: {date_range['max']}\")\n",
    "print(f\"Date span: {(date_range['max'] - date_range['min']).days} days\")\n",
    "\n",
    "# Check for articles with invalid dates\n",
    "invalid_dates = df[df['published_dt'].isnull()]\n",
    "if len(invalid_dates) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(invalid_dates)} articles with invalid dates\")\n",
    "\n",
    "# Check for stale data (>365 days old per ¬ß11 auto-expand rule)\n",
    "now = pd.Timestamp.now()\n",
    "stale_cutoff = now - pd.Timedelta(days=365)\n",
    "stale_articles = df[df['published_dt'] < stale_cutoff]\n",
    "print(f\"\\n‚ö†Ô∏è Stale articles (>365 days old): {len(stale_articles)} ({len(stale_articles)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Articles per month\n",
    "df_valid_dates = df[df['published_dt'].notna()].copy()\n",
    "df_valid_dates['month'] = df_valid_dates['published_dt'].dt.to_period('M')\n",
    "articles_per_month = df_valid_dates.groupby('month').size()\n",
    "\n",
    "# Plot temporal distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "articles_per_month.plot(kind='bar', ax=ax, edgecolor='black')\n",
    "ax.set_title('Articles per Month')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Articles per month:\\n{articles_per_month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78526a4",
   "metadata": {},
   "source": [
    "## 5. Source Distribution (¬ß11: Metadata-First Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c314d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source diversity check\n",
    "print(\"=== Source Distribution ===\")\n",
    "source_counts = df['source'].value_counts()\n",
    "source_pct = (source_counts / len(df) * 100).round(2)\n",
    "\n",
    "print(f\"Total unique sources: {len(source_counts)}\")\n",
    "print(f\"\\nTop 10 sources:\")\n",
    "print(source_pct.head(10))\n",
    "\n",
    "# Detect over-represented sources (>30% concentration)\n",
    "dominant_sources = source_pct[source_pct > 30]\n",
    "if len(dominant_sources) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Dominant sources (>30%):\")\n",
    "    print(dominant_sources)\n",
    "else:\n",
    "    print(f\"\\n‚úì No single source dominates (all <30%)\")\n",
    "\n",
    "# Plot source distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "source_pct.head(15).plot(kind='barh', ax=ax, edgecolor='black')\n",
    "ax.set_title('Top 15 Sources by Article Count (%)')\n",
    "ax.set_xlabel('Percentage of Total Articles')\n",
    "ax.axvline(30, color='red', linestyle='--', label='Dominance threshold (30%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201303f2",
   "metadata": {},
   "source": [
    "## 6. Language Distribution (¬ß15: Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language validation\n",
    "print(\"=== Language Distribution ===\")\n",
    "lang_dist = df['language'].value_counts()\n",
    "print(lang_dist)\n",
    "\n",
    "# Check language code format (should be ISO 639-1: 2 chars)\n",
    "invalid_lang_codes = df[df['language'].str.len() != 2]\n",
    "if len(invalid_lang_codes) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(invalid_lang_codes)} articles with invalid language codes\")\n",
    "    print(invalid_lang_codes[['title', 'language']].head())\n",
    "else:\n",
    "    print(f\"\\n‚úì All language codes are valid (2 chars)\")\n",
    "\n",
    "# Non-English articles\n",
    "non_english = df[df['language'] != 'en']\n",
    "print(f\"\\nNon-English articles: {len(non_english)} ({len(non_english)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(non_english) > 0:\n",
    "    print(\"‚ö†Ô∏è Note: Ensure embedding model supports these languages\")\n",
    "    print(non_english['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371045c",
   "metadata": {},
   "source": [
    "## 7. Chunking Preview (¬ß11: Chunking Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaaa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate chunking params from config/rag.yml\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 900  # from config/rag.yml\n",
    "chunk_overlap = 120\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"=== Chunking Preview (size={chunk_size}, overlap={chunk_overlap}) ===\")\n",
    "\n",
    "# Test on sample articles\n",
    "sample_size = min(10, len(df))\n",
    "sample_texts = df.sample(sample_size)['text']\n",
    "chunks_per_article = sample_texts.apply(lambda x: len(splitter.split_text(x)))\n",
    "\n",
    "print(f\"Sample size: {sample_size} articles\")\n",
    "print(f\"Avg chunks per article: {chunks_per_article.mean():.1f}\")\n",
    "print(f\"Median chunks: {chunks_per_article.median():.1f}\")\n",
    "print(f\"Max chunks in sample: {chunks_per_article.max()}\")\n",
    "\n",
    "# Estimate total chunks for full corpus\n",
    "estimated_total_chunks = int(chunks_per_article.mean() * len(df))\n",
    "print(f\"\\nüìä Estimated total chunks for {len(df)} articles: ~{estimated_total_chunks}\")\n",
    "print(f\"   (at ~1500 chunks/batch, ~{np.ceil(estimated_total_chunks/1500):.0f} batches)\")\n",
    "\n",
    "# Distribution of chunks per article\n",
    "chunks_per_article.hist(bins=10, edgecolor='black', figsize=(10, 5))\n",
    "plt.title('Chunks per Article Distribution (Sample)')\n",
    "plt.xlabel('Number of Chunks')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(chunks_per_article.mean(), color='red', linestyle='--', label=f'Mean: {chunks_per_article.mean():.1f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c0f63",
   "metadata": {},
   "source": [
    "## 8. URL Integrity (¬ß12: URL Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41022d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL validation\n",
    "print(\"=== URL Integrity ===\")\n",
    "\n",
    "# Check URL format\n",
    "df['url_valid'] = df['url'].str.match(r'^https?://', na=False)\n",
    "invalid_urls = df[~df['url_valid']]\n",
    "\n",
    "print(f\"Total URLs: {len(df)}\")\n",
    "print(f\"Valid URLs (http/https): {df['url_valid'].sum()}\")\n",
    "print(f\"Invalid URLs: {len(invalid_urls)}\")\n",
    "\n",
    "if len(invalid_urls) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Found {len(invalid_urls)} invalid URLs\")\n",
    "    print(invalid_urls[['title', 'url']].head())\n",
    "\n",
    "# Check for tracking params that should've been stripped\n",
    "tracking_params = ['utm_', 'mc_cid', 'mc_eid', 'ref', 'fbclid', 'gclid']\n",
    "has_tracking = df['url'].str.contains('|'.join(tracking_params), regex=True, na=False)\n",
    "\n",
    "if has_tracking.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {has_tracking.sum()} URLs still contain tracking params\")\n",
    "    print(\"   This may indicate URL normalization issues\")\n",
    "    print(df[has_tracking][['url']].head())\n",
    "else:\n",
    "    print(f\"\\n‚úì All URLs are properly normalized (no tracking params)\")\n",
    "\n",
    "# Check for duplicate URLs\n",
    "duplicate_urls = df['url'].duplicated().sum()\n",
    "print(f\"\\n{'‚ö†Ô∏è' if duplicate_urls > 0 else '‚úì'} Duplicate URLs: {duplicate_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da29183",
   "metadata": {},
   "source": [
    "## 9. Content Hash Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify content_hash integrity (sample check)\n",
    "if 'content_hash' in df.columns:\n",
    "    print(\"=== Content Hash Verification ===\")\n",
    "    \n",
    "    def verify_hash(row):\n",
    "        \"\"\"Check if content_hash matches actual text.\"\"\"\n",
    "        if pd.isna(row['content_hash']) or pd.isna(row['text']):\n",
    "            return False\n",
    "        expected = hashlib.sha1(row['text'].encode()).hexdigest()\n",
    "        return row['content_hash'] == expected\n",
    "    \n",
    "    # Check a sample (checking all can be slow)\n",
    "    sample_size = min(100, len(df))\n",
    "    sample = df.sample(sample_size)\n",
    "    \n",
    "    hash_matches = sample.apply(verify_hash, axis=1)\n",
    "    mismatches = sample[~hash_matches]\n",
    "    \n",
    "    print(f\"Sample size: {sample_size}\")\n",
    "    print(f\"Hash matches: {hash_matches.sum()} ({hash_matches.sum()/sample_size*100:.1f}%)\")\n",
    "    \n",
    "    if len(mismatches) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: {len(mismatches)} hash mismatches detected\")\n",
    "        print(\"   This may indicate data corruption\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì All sampled hashes are valid\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è content_hash field not found in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0598aa9",
   "metadata": {},
   "source": [
    "## 10. Metadata Richness & Authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check optional field coverage\n",
    "print(\"=== Metadata Richness ===\")\n",
    "\n",
    "if 'authors' in df.columns:\n",
    "    df['has_authors'] = df['authors'].apply(\n",
    "        lambda x: len(x) > 0 if isinstance(x, list) else (pd.notna(x) and x != '')\n",
    "    )\n",
    "    author_coverage = df['has_authors'].mean() * 100\n",
    "    print(f\"Articles with authors: {df['has_authors'].sum()} ({author_coverage:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'authors' field not found\")\n",
    "\n",
    "# Check for sparse metadata\n",
    "sparse_metadata = df[\n",
    "    (df['title'].str.len() < 20) | \n",
    "    (df['source'].isnull())\n",
    "]\n",
    "print(f\"\\nArticles with sparse metadata: {len(sparse_metadata)} ({len(sparse_metadata)/len(df)*100:.1f}%)\")\n",
    "if len(sparse_metadata) > 0:\n",
    "    print(\"   (title <20 chars OR missing source)\")\n",
    "\n",
    "# Title length distribution\n",
    "df['title_length'] = df['title'].str.len()\n",
    "print(f\"\\nTitle length stats:\")\n",
    "print(df['title_length'].describe())\n",
    "\n",
    "# Plot metadata completeness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Title length\n",
    "df['title_length'].hist(bins=20, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Title Length Distribution')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].axvline(20, color='red', linestyle='--', label='Min threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Author coverage\n",
    "if 'has_authors' in df.columns:\n",
    "    df['has_authors'].value_counts().plot(kind='bar', ax=axes[1], edgecolor='black')\n",
    "    axes[1].set_title('Author Field Coverage')\n",
    "    axes[1].set_xticklabels(['No Authors', 'Has Authors'], rotation=0)\n",
    "    axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62726c1d",
   "metadata": {},
   "source": [
    "## 11. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc021e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect extreme articles\n",
    "print(\"=== Outlier Detection ===\")\n",
    "\n",
    "# Define thresholds\n",
    "q01 = df['text_length'].quantile(0.01)\n",
    "q99 = df['text_length'].quantile(0.99)\n",
    "\n",
    "outliers = df[\n",
    "    (df['text_length'] > q99) |  # Very long\n",
    "    (df['text_length'] < q01)     # Very short\n",
    "]\n",
    "\n",
    "print(f\"Outliers (1st & 99th percentile): {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Very short (<{q01:.0f} chars): {len(df[df['text_length'] < q01])}\")\n",
    "print(f\"  Very long (>{q99:.0f} chars): {len(df[df['text_length'] > q99])}\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(f\"\\nSample outliers:\")\n",
    "    print(outliers[['title', 'text_length', 'word_count', 'source']].head(10))\n",
    "    \n",
    "# Box plot for outliers\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "df.boxplot(column='text_length', by='source', ax=ax, rot=45)\n",
    "ax.set_title('Text Length by Source (Boxplot for Outlier Detection)')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Text Length (chars)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e886e78",
   "metadata": {},
   "source": [
    "## 12. Summary: Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82822c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive validation report\n",
    "validation_report = {\n",
    "    'total_articles': len(df),\n",
    "    'date_range_days': (date_range['max'] - date_range['min']).days if 'date_range' in locals() else None,\n",
    "    'unique_sources': len(source_counts) if 'source_counts' in locals() else None,\n",
    "    'median_text_length': int(df['text_length'].median()),\n",
    "    'median_word_count': int(df['word_count'].median()),\n",
    "    'short_articles_pct': round(len(short_articles) / len(df) * 100, 2) if 'short_articles' in locals() else None,\n",
    "    'duplicate_hashes': duplicate_hashes if 'duplicate_hashes' in locals() else None,\n",
    "    'stale_articles_pct': round(len(stale_articles) / len(df) * 100, 2) if 'stale_articles' in locals() else None,\n",
    "    'invalid_urls': len(invalid_urls) if 'invalid_urls' in locals() else None,\n",
    "    'non_english_pct': round(len(non_english) / len(df) * 100, 2) if 'non_english' in locals() else None,\n",
    "    'estimated_total_chunks': estimated_total_chunks if 'estimated_total_chunks' in locals() else None,\n",
    "    'author_coverage_pct': round(author_coverage, 2) if 'author_coverage' in locals() else None,\n",
    "    'outliers_pct': round(len(outliers) / len(df) * 100, 2) if 'outliers' in locals() else None,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION REPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for key, value in validation_report.items():\n",
    "    print(f\"{key.replace('_', ' ').title():.<40} {value}\")\n",
    "\n",
    "# Export to CSV\n",
    "report_df = pd.Series(validation_report)\n",
    "report_path = '../data/processed/validation_report.csv'\n",
    "report_df.to_csv(report_path, header=['value'])\n",
    "print(f\"\\n‚úì Report saved to: {report_path}\")\n",
    "\n",
    "# Decision points\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION POINTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'short_articles' in locals() and len(short_articles) / len(df) > 0.05:\n",
    "    print(\"‚ö†Ô∏è  >5% short articles ‚Üí Consider re-scraping with stricter min_chars\")\n",
    "    \n",
    "if 'date_range' in locals() and (date_range['max'] - date_range['min']).days < 30:\n",
    "    print(\"‚ö†Ô∏è  Date range <30 days ‚Üí Increase scraping frequency\")\n",
    "    \n",
    "if 'source_pct' in locals() and (source_pct.max() > 50):\n",
    "    print(f\"‚ö†Ô∏è  One source >{source_pct.max():.0f}% ‚Üí Add more feeds to feeds.yml\")\n",
    "    \n",
    "if 'stale_articles' in locals() and len(stale_articles) / len(df) > 0.6:\n",
    "    print(\"‚ö†Ô∏è  Stale articles >60% ‚Üí Consider filtering before indexing\")\n",
    "    \n",
    "if 'chunks_per_article' in locals() and chunks_per_article.mean() > 10:\n",
    "    print(\"‚ö†Ô∏è  Avg >10 chunks/article ‚Üí Consider reducing chunk_size to 700\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to proceed with indexing if no critical warnings above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b784d4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After validation, proceed with indexing:\n",
    "\n",
    "```bash\n",
    "python -m data_pipeline.pipeline index \\\n",
    "  --input data/processed/articles_clean.jsonl \\\n",
    "  --cfg config/rag.yml\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Chunk articles (900 chars, 120 overlap)\n",
    "- Generate embeddings via OpenAI\n",
    "- Store in ChromaDB at `data/vdb/chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ad3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary stats\n",
    "print(\"üéØ EDA Complete!\")\n",
    "print(f\"üìä Analyzed {len(df)} articles\")\n",
    "print(f\"üìÖ Date range: {(date_range['max'] - date_range['min']).days} days\" if 'date_range' in locals() else \"\")\n",
    "print(f\"üåê Sources: {len(source_counts)}\" if 'source_counts' in locals() else \"\")\n",
    "print(f\"üìù Estimated chunks: ~{estimated_total_chunks}\" if 'estimated_total_chunks' in locals() else \"\")\n",
    "print(\"\\n‚úÖ Review the validation report and decision points above before indexing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
